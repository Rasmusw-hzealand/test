{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rasmusw-hzealand/test/blob/main/Kopi_af_ChatGPT_Chat_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat sequence"
      ],
      "metadata": {
        "id": "MgpnjDz_ARFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install OpenAI components"
      ],
      "metadata": {
        "id": "bmZomqAqUet4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMZtZ_fFULYL",
        "outputId": "99ce1fef-e8e9-4461-e347-2c48efcc660f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.50.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtwUjrS2VbqJ",
        "outputId": "fc7feaf3-9fa2-4fa3-d452-342e5282553f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive"
      ],
      "metadata": {
        "id": "LiVhUlZfVsRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERwSE0yEV4Rs",
        "outputId": "b64cda23-c4a3-4329-fba0-4a6ac7e08683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load .env file"
      ],
      "metadata": {
        "id": "cc3ly5uAWIYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('drive/My Drive/Colab Notebooks/env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5ycZHwQWNan",
        "outputId": "9e576985-3174-49c0-aee5-b71c472e17f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialize Open AI client"
      ],
      "metadata": {
        "id": "uWB5Q5NqXCYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI as openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.api_key\n",
        "client = openai()"
      ],
      "metadata": {
        "id": "NRygMPIVXDzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Various variables and settings"
      ],
      "metadata": {
        "id": "rerQJqKxX_i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gptModel=\"gpt-4o-mini\"\n",
        "chatCompletionChoices=1\n",
        "samplingTemperature=None\n",
        "maxCompletionTokens=100\n",
        "nucleusSampling=None"
      ],
      "metadata": {
        "id": "WmrMIHu-ZwNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1st prompt"
      ],
      "metadata": {
        "id": "A4nb_Yj2Z_1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InitialSystemContent = \"You are a helpful teacher.\"\n",
        "#InitialSystemContent = \"You are an experienced sales copywriter. The name of my business is Future Mind Consulting, and we sell software development. Write a 200-word email that will persuade anyone who reads it to become a customer.\"\n"
      ],
      "metadata": {
        "id": "qGt6k0aqYxCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstUserContent = \"Are there other measures than time complexity for an algorithm?\"\n",
        "#FirstUserContent = \"Serious\""
      ],
      "metadata": {
        "id": "3Q13yuwTY4zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "WL5KOl_KZUFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgSDj-dyaVNe",
        "outputId": "6fb34c32-8bf2-42c9-9f02-8e4b1eb8e469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "EeCD7AUFZlrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something:  FirstAssistantContent = \"Yes, there are other measures besides time complexfor an algorithm, such as space complexity.\"\n"
      ],
      "metadata": {
        "id": "aSHZU_1mbPJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(FirstAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ppulUHVbb1W",
        "outputId": "fd4c1bd6-baa0-4733-e4f3-4f32b5539696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, there are several measures and considerations beyond time complexity when evaluating the performance and efficiency of an algorithm. Some of the important ones include:\n",
            "\n",
            "1. **Space Complexity**: This measures the amount of memory an algorithm uses in relation to the size of the input. It includes both the temporary space allocated during the execution of the algorithm and the space needed to store the input data.\n",
            "\n",
            "2. **Stability**: For sorting algorithms, stability refers to whether equal elements retain their relative order after sorting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2nd prompt"
      ],
      "metadata": {
        "id": "RFqIPlLuflNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SecondUserContent = \"What is it?\"\n",
        "#SecondUserContent = \"It is for medical device industry\""
      ],
      "metadata": {
        "id": "r_RsKgpPfqEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "ZvDPMcZegS4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MNGcNulhYNy",
        "outputId": "651617c3-75d9-4ef9-90c9-3f86bd3c61e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}, {'role': 'assistant', 'content': 'Yes, there are several measures and considerations beyond time complexity when evaluating the performance and efficiency of an algorithm. Some of the important ones include:\\n\\n1. **Space Complexity**: This measures the amount of memory an algorithm uses in relation to the size of the input. It includes both the temporary space allocated during the execution of the algorithm and the space needed to store the input data.\\n\\n2. **Stability**: For sorting algorithms, stability refers to whether equal elements retain their relative order after sorting.'}, {'role': 'user', 'content': 'What is it?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "XCQaA_8chjk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SecondAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something: SecondAssistantContent = \"<something>\""
      ],
      "metadata": {
        "id": "io-1QG3jhq_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SecondAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJeh1LFthwQk",
        "outputId": "e15aee5c-2ec5-42e5-88b0-e3711e360a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stability in sorting algorithms refers to the property that when two elements have equal keys (or values) and one element appears before the other in the original input, a stable sort will ensure that this order is preserved in the sorted output. In other words, if the sorting algorithm is stable, the relative order of equivalent elements remains unchanged.\n",
            "\n",
            "### Example:\n",
            "Consider an array of objects where each object represents a person with a name and age:\n",
            "\n",
            "```plaintext\n",
            "[\n",
            "    {name: 'Alice', age:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3rd prompt"
      ],
      "metadata": {
        "id": "n1OwZfAhkuBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ThirdUserContent = \"<some content>\""
      ],
      "metadata": {
        "id": "oJLEhjbSk8gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": SecondAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": ThirdUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "IgcFKH1YlOu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUY_FAiblngZ",
        "outputId": "90b3fcf5-cb82-4272-d80c-a58d2015302b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}, {'role': 'assistant', 'content': 'Yes, there are several measures and considerations beyond time complexity when evaluating the performance and efficiency of an algorithm. Some of the important ones include:\\n\\n1. **Space Complexity**: This measures the amount of memory an algorithm uses in relation to the size of the input. It includes both the temporary space allocated during the execution of the algorithm and the space needed to store the input data.\\n\\n2. **Stability**: For sorting algorithms, stability refers to whether equal elements retain their relative order after sorting.'}, {'role': 'user', 'content': 'What is it?'}, {'role': 'assistant', 'content': \"Stability in sorting algorithms refers to the property that when two elements have equal keys (or values) and one element appears before the other in the original input, a stable sort will ensure that this order is preserved in the sorted output. In other words, if the sorting algorithm is stable, the relative order of equivalent elements remains unchanged.\\n\\n### Example:\\nConsider an array of objects where each object represents a person with a name and age:\\n\\n```plaintext\\n[\\n    {name: 'Alice', age:\"}, {'role': 'user', 'content': '<some content>'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature,max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "dxvc-yxCl1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ThirdAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something: ThirdAssistantContent = \"<something>\""
      ],
      "metadata": {
        "id": "qrluUrmfmC7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ThirdAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFjLzydvmOmb",
        "outputId": "2b560a22-8792-4f68-9ff2-981a945013dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It appears that your message may have been cut off or is incomplete. If you have a specific question or further content you'd like to share or discuss regarding stability, sorting algorithms, or any other topic, please feel free to provide that information, and I'll be glad to assist you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises"
      ],
      "metadata": {
        "id": "p1fwf3DB_lOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answering the questions below you may benefit from these references:  \n",
        "\n",
        "\n",
        "1.   Chat completion parameters : https://platform.openai.com/docs/api-reference/chat/create\n",
        "2.   ChatGPT Prompts Library: https://gptbot.io/chatgpt-prompts/\n",
        "3.   Models overview: https://platform.openai.com/docs/models\n",
        "\n"
      ],
      "metadata": {
        "id": "XUSB9DnTBki3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Establish your own chat sequence with max 3. prompts (unless you extend the notebook with additional prompts). You may start with a prompt from the  'ChatGPT Prompts Library' as mentioned above.  \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "bhE77NaXAwzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "LQqJml-opqv4",
        "outputId": "7b9b78cf-ca4a-43dc-a041-22105b69935d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.50.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv"
      ],
      "metadata": {
        "id": "pQw5lUfrp6Oi",
        "outputId": "e58fdee8-fc78-417e-8e6a-018d2b8de207",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5XUlChlFp_30",
        "outputId": "c4a44574-bf15-4b3f-d197-08ab9d46162b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('drive/My Drive/Colab Notebooks/env')"
      ],
      "metadata": {
        "id": "_zikefZWqD0q",
        "outputId": "6bb3b1e5-8e4d-444e-82e7-905c9f895c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI as openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.api_key\n",
        "client = openai()"
      ],
      "metadata": {
        "id": "O-J1WmvHqHbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptModel=\"gpt-4o-mini\"\n",
        "chatCompletionChoices=1\n",
        "samplingTemperature=None\n",
        "maxCompletionTokens=100\n",
        "nucleusSampling=None"
      ],
      "metadata": {
        "id": "OZeKWVt3qKCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "InitialSystemContent = \"You are a helpful maid.\""
      ],
      "metadata": {
        "id": "OjGUJCQOqlvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstUserContent = \"how many numbers are there in map coodinates?\""
      ],
      "metadata": {
        "id": "JlMgKVhLqrO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "y-1CxOObq7EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "id": "6rcFA5oyq_N6",
        "outputId": "9f68158b-245b-4e4c-8919-80a56faac95d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful maid.'}, {'role': 'user', 'content': 'how many numbers are there in map coodinates?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "RzU3cpWerE5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstAssistantContent = response.choices[0].message.content"
      ],
      "metadata": {
        "id": "M7yBn-PxrIsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(FirstAssistantContent)"
      ],
      "metadata": {
        "id": "dyL9GSp8rLlU",
        "outputId": "aec19824-4348-4f09-bd99-fbb1c9c1bac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map coordinates generally refer to the numerical values that represent a specific location on Earth. Typically, there are two main types of coordinates used in mapping systems:\n",
            "\n",
            "1. **Geographic Coordinates**: These consist of two numbers that represent the latitude and longitude of a location. Latitude indicates how far north or south a point is from the equator, while longitude indicates how far east or west a point is from the Prime Meridian. For example:\n",
            "   - Latitude: 40.7128° N\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SecondUserContent = \"how many decimals are there in map coodinates?\""
      ],
      "metadata": {
        "id": "i9lVceudrQxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "-Xbu_PUOrjFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "id": "-gtO3s-hrwvc",
        "outputId": "bb5393c3-74e9-4f18-9eb9-f27bad359965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful maid.'}, {'role': 'user', 'content': 'how many numbers are there in map coodinates?'}, {'role': 'assistant', 'content': 'Map coordinates generally refer to the numerical values that represent a specific location on Earth. Typically, there are two main types of coordinates used in mapping systems:\\n\\n1. **Geographic Coordinates**: These consist of two numbers that represent the latitude and longitude of a location. Latitude indicates how far north or south a point is from the equator, while longitude indicates how far east or west a point is from the Prime Meridian. For example:\\n   - Latitude: 40.7128° N\\n  '}, {'role': 'user', 'content': 'how many decimals are there in map coodinates?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "AZv7XCLbr16k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SecondAssistantContent = response.choices[0].message.content"
      ],
      "metadata": {
        "id": "eRc4QXMlr5Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SecondAssistantContent)"
      ],
      "metadata": {
        "id": "O3btfpwDr-1h",
        "outputId": "4cc0055b-3099-4178-d5f5-a7057e319ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map coordinates can vary in the number of decimal places used, depending on the level of precision required. Here's a breakdown:\n",
            "\n",
            "1. **Two Decimal Places**: This provides an approximation of about 1.11 kilometers (0.69 miles). It can locate an area roughly the size of a city.\n",
            "\n",
            "2. **Three Decimal Places**: This improves accuracy to about 111 meters (364 feet). It can pinpoint a location within a neighborhood.\n",
            "\n",
            "3. **Four Decimal Places**: This offers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YlV_25_7KDu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Does it make any difference, if you force assistants 'content' to be somethings else, than that received in the previous reply from ChatGPT. Technically you can do this removing one or more '#Force assitant message to be something:' and specify something else.    \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "mXU-PEtZA0Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nej det gøre ikke nogle forskel"
      ],
      "metadata": {
        "id": "3qWAmZ7rwTPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the 'model' parameter specified by the 'gptModel' variable.      \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "m0fNJEjSG5Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Her er, hvad 'model'-parameteren gør:\n",
        "\n",
        "Modelvalg: Den bestemmer, hvilken version af modellen der bruges. For eksempel kan du sætte 'gptModel' til \"gpt-4\", \"gpt-3.5-turbo\" eller andre modelnavne afhængigt af den specifikke opgave, kontekst eller de beregningsmæssige krav.\n",
        "\n",
        "Ydelse og omkostninger: Forskellige modeller kan have variationer i ydelse, hastighed og omkostninger. For eksempel kan nyere modeller som GPT-4 være mere præcise, men dyrere at bruge, mens GPT-3.5-turbo kan være hurtigere og mere omkostningseffektiv, men en smule mindre avanceret.\n",
        "\n",
        "Opgaveegnethed: Afhængigt af opgaven (såsom opsummering, kodegenerering eller uformel samtale) kan én model præstere bedre end andre. 'Model'-parameteren sikrer, at den mest passende model bruges til den aktuelle opgave."
      ],
      "metadata": {
        "id": "9lIojz530ORQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the 'messages' parameter specified by the 'chatMessages' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "MnyEfPT3HkRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parametret messages, som er specificeret af variablen chatMessages, bruges til at give modellen samtalehistorik eller kontekst. Det gør det muligt for modellen at forstå den igangværende samtale og svare i overensstemmelse hermed. chatMessages indeholder en liste af beskeder, hvor hver besked har en bestemt struktur, der angiver, hvem der taler (rolle), og hvad der bliver sagt (indhold).\n",
        "\n",
        "Struktur af messages Parameteren\n",
        "Hver besked i listen er en ordbog (dictionary) med to hovednøgler:\n",
        "\n",
        "role: Angiver, hvem der sender beskeden. Dette kan være \"system\", \"user\", eller \"assistant\".\n",
        "\n",
        "system: Bruges til at fastsætte konteksten eller reglerne for samtalen. Dette er typisk en introduktion, hvor man kan definere modellens rolle.\n",
        "user: Dette er beskeder fra brugeren, altså de spørgsmål eller instruktioner, som brugeren sender til modellen.\n",
        "assistant: Dette er modellens svar, hvor den agerer som en assistent.\n",
        "content: Den faktiske beskedtekst, som udgør indholdet af samtalen.\n",
        "\n",
        "Eksempel på Struktur af messages\n",
        "python\n",
        "Kopier kode\n",
        "chatMessages = [\n",
        "    {\"role\": \"system\", \"content\": \"Du er en hjælpsom assistent.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hvad er hovedstaden i Frankrig?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Hovedstaden i Frankrig er Paris.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Kan du forklare mere om Paris?\"},\n",
        "]\n",
        "I dette eksempel:\n",
        "\n",
        "system-beskeden sætter scenen og fortæller modellen, at den skal opføre sig som en hjælpsom assistent.\n",
        "user-beskeder er brugerens input, som stiller spørgsmål eller beder om information.\n",
        "assistant-beskeder er modellens svar, baseret på brugerens input og konteksten.\n",
        "Formålet med messages er at skabe en flydende samtale, hvor tidligere beskeder bevares for at sikre sammenhæng i modellens svar."
      ],
      "metadata": {
        "id": "dcj5cmhf0njA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain the 'n' parameter specified by the 'chatCompletionChoices' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "W0ipFRpcH4-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "n-parameteren, som er specificeret af variablen chatCompletionChoices, angiver, hvor mange forskellige svar (eller muligheder) modellen skal generere som svar på en forespørgsel.\n",
        "\n",
        "Når du sætter n til en værdi, f.eks. 3, vil modellen returnere 3 forskellige svarforslag på den samme besked. Dette kan være nyttigt, hvis du vil se flere mulige svar og vælge det, der passer bedst til din sammenhæng.\n",
        "\n",
        "Eksempel:\n",
        "python\n",
        "Kopier kode\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=chatMessages,\n",
        "    n=3\n",
        ")\n",
        "I dette eksempel vil modellen returnere 3 forskellige svarmuligheder, som alle er baseret på det samme input i chatMessages."
      ],
      "metadata": {
        "id": "VMYOt0Xc1Y2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the 'temperature' parameter specified by the 'samplingTemperature' variable.      \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "Q2RxcWOzIVM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "temperature-parameteren, som er specificeret af variablen samplingTemperature, styrer modellens kreativitet og variation i svarene.\n",
        "\n",
        "En lav temperature-værdi (f.eks. 0.2) gør, at modellen giver mere fokuserede, forudsigelige og præcise svar. Det er nyttigt til opgaver, der kræver klare og faktuelle svar.\n",
        "En høj temperature-værdi (f.eks. 0.8) gør svarene mere kreative og varierede. Det er bedre egnet til opgaver, der kræver kreativ skrivning eller hvor flere mulige løsninger kan være relevante.\n",
        "Med andre ord bestemmer temperature, hvor \"tilfældige\" eller \"risikovillige\" modellens svar skal være."
      ],
      "metadata": {
        "id": "DSuRA8mQ1jh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain the 'max_completion_tokens' parameter specified by the 'maxCompletionTokens' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "MwNggaKXIrFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_completion_tokens-parameteren, som er specificeret af variablen maxCompletionTokens, angiver det maksimale antal tokens (enheder af tekst) modellen kan generere i sit svar.\n",
        "\n",
        "Dette sætter en grænse for, hvor langt modellens svar kan være. En token kan være så kort som ét tegn eller så langt som ét ord, afhængigt af sprogstrukturen.\n",
        "Hvis du sætter en lav værdi for maxCompletionTokens, vil modellens svar være kortere, mens en højere værdi tillader længere og mere detaljerede svar.\n",
        "Denne parameter er nyttig, hvis du vil kontrollere længden af svarene eller begrænse dem af hensyn til tid, omkostninger eller specifikke opgaver."
      ],
      "metadata": {
        "id": "TJfIg34h1thO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain the 'top_p' parameter specified by the 'nucleusSampling' variable.      \n",
        "Answer: ?    "
      ],
      "metadata": {
        "id": "QW7wpSdKI_ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_p-parameteren, som er specificeret af variablen nucleusSampling, styrer brugen af nucleus sampling, en teknik til at begrænse modellens svar til de mest sandsynlige muligheder.\n",
        "\n",
        "Når top_p er sat til en værdi som 0.9, betyder det, at modellen vælger blandt de mest sandsynlige svar, indtil den akkumulerede sandsynlighed når 90%. Det hjælper med at sikre, at svarene er rimeligt varierede, men stadig sandsynlige.\n",
        "Hvis top_p er sat til 1, vil modellen vælge fra hele fordelingen af mulige svar, hvilket kan gøre svarene mere tilfældige.\n",
        "En lavere top_p værdi (f.eks. 0.5) vil begrænse valgene til kun de mest sandsynlige ord eller sætninger, hvilket giver mere fokuserede og præcise svar.\n",
        "Kort sagt bestemmer top_p, hvor mange af de mest sandsynlige svarmuligheder modellen kan vælge fra, hvilket påvirker balancen mellem kreativitet og præcision i genereringen af svar."
      ],
      "metadata": {
        "id": "LD3Ci64B2AL7"
      }
    }
  ]
}